{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Infection Risk Prediction with Amazon SageMaker Autopilot\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#SetupAPjob)\n",
    "1. [Autopilot Results](#Results)\n",
    "1. [Batch Inference](#BatchInf)\n",
    "1. [Cleanup](#Cleanup)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (hence the name) or with human guidance, without code through SageMaker Studio, or using the AWS SDKs. This notebook, as a first glimpse, will use the AWS SDKs to simply create and deploy a machine learning model.\n",
    "\n",
    "Coronavirus disease (COVID-19) is an infectious disease caused by a new virus. People can catch COVID-19 from others who have the virus. While we are still learning about how COVID-19 affects people, older persons and persons with pre-existing medical conditions (such as high blood pressure, heart disease, lung disease, cancer or diabetes) appear to develop serious illness more often than others.\n",
    "\n",
    "In this notebook we are going to demonstrate how we can use AutoPilot to create a model to predict COVID-19 infection risk based on various factors like age, sex, country, present medical conditions, number of people contacted with, steps taken by the government to contain the risk, etc.   \n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "\n",
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd \n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import s3fs\n",
    "import boto3\n",
    "import sagemaker\n",
    "import io\n",
    "from datetime import timedelta\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# You can modify the following to use a bucket of your choosing\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-autopilot-covid19'\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# This is the client we will use to interact with SageMaker AutoPilot\n",
    "sm = boto3.Session().client(service_name='sagemaker',region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to use the data which is generated by a public survey which captures various data like age, sex, country, health conditions and other relevant data along with labels like probability of covid-19 infection and mortality. In this example we are going to use probability of covid-19 infection (column name: risk_infection) as our target column.\n",
    "\n",
    "The dataset we use is publicly available at https://www.covid19survivalcalculator.com/download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the csv file to local\n",
    "!wget https://www.covid19survivalcalculator.com/data/master_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# capture only the columns \n",
    "df = pd.read_csv('master_dataset.csv', nrows=0)\n",
    "tempheaderdf = pd.DataFrame(df)\n",
    "tempheaderdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the rows, assign column names to the data and drop columns we are not going to use in this notebook \n",
    "#(you can modify this notebook to suit your needs)\n",
    "df = pd.read_csv('master_dataset.csv', parse_dates=True, header=None, skiprows=[0])\n",
    "pd.set_option('display.max_columns', 50)\n",
    "df.drop(df.columns[[43]], axis = 1, inplace = True)\n",
    "df.columns = tempheaderdf.columns\n",
    "df.drop(['survey_date', 'region', 'ip_accuracy', 'prescription_medication', 'risk_mortality'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the columns we are going to use for this notebook:\n",
    "- `country`: two-letter abbreviation for country; for example, US or GB\n",
    "- `ip_latitude`: IP location latitude randomly adjusted (-0.1 to +0.1) as per the download website\n",
    "- `ip_longitude`: IP location longitude randomly adjusted (-0.1 to +0.1) as per the download website\n",
    "- `sex`: Sex of the individual\n",
    "- `age`: Age of the individual reduced to 10-year bandings as per the download website\n",
    "- `height`: Height of the individual\n",
    "- `weight`: Weight of the individual\n",
    "- `bmi`: BMI (Body Mass Index) of the individual\n",
    "- `blood_type`: Blood group\n",
    "- `smoking`: Does the individual smoke or not\n",
    "- `alcohol`: Does the individual consume alcohol or not\n",
    "- `cannabis`: Does the individual use Non-Prescription Drugs (cannabis) or not\n",
    "- `amphetamines`: Does the individual use Non-Prescription Drugs (amphetamines) or not\n",
    "- `cocaine`: Does the individual use Non-Prescription Drugs (cocaine) or not\n",
    "- `lsd`: Does the individual use Non-Prescription Drugs (lsd) or not\n",
    "- `mdma`: Does the individual use Non-Prescription Drugs (mdma) or not\n",
    "- `contacts_count`: How many people was the individual in close contact with in the last week? (including partners, children, work colleagues, customers, patients etc.)\n",
    "- `house_count`: How many people live in individual's house / flat\n",
    "- `text_working`: Are traveling to work / school?\n",
    "- `rate_government_action`: Is the individual's country's government taking COVID-19 seriously\n",
    "- `rate_reducing_risk_single`: Is the individual taking steps to reduce his/her risk (social distancing, washing hands).\n",
    "- `rate_reducing_risk_house`: Do the people the individual live with are taking steps to reduce risk (social distancing, washing hands).\n",
    "- `rate_reducing_mask`: Does the individual wear a mask when outside of house / flat?\n",
    "- `covid19_positive`: Is the individual covid19 positive\n",
    "- `covid19_symptoms`: Does the individual have covid19 symptoms (dry cough, fever, breathlessness etc)\n",
    "- `covid19_contact`: Is the individual come in contact with a person having covid19\n",
    "- `asthma`: Does the individual have asthma\n",
    "- `kidney_disease`: Does the individual have kidney disease\n",
    "- `compromised_immune`: Does the individual have compromised immune system\n",
    "- `heart_disease`: Does the individual have heart disease\n",
    "- `lung_disease`: Does the individual have lung disease\n",
    "- `diabetes`: Does the individual have diabetes\n",
    "- `hiv_positive`: Is the individual hiv positive\n",
    "- `hypertension`: Does the individual have hypertension\n",
    "- `other_chronic`: Does the individual have any other chronic\n",
    "- `opinion_infection`: Individual's own opinion of his/her chances of getting COVID-19\n",
    "- `opinion_mortality`: Individual's own opinion of his/her chances of dying if they have COVID-19\n",
    "- `risk_infection`: Probaility of getting infected with COVID-19 (<b>Target Attribute/Column</b>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of the data for the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['risk_infection'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "axarr = df.hist(column='risk_infection', figsize=(15,5))\n",
    "for ax in axarr.flatten():\n",
    "    ax.set_xlabel(\"Risk Infection Probability\")\n",
    "    ax.set_ylabel(\"Records in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for training and testing\n",
    "\n",
    "Divide the data into training and testing splits. The training split is used by SageMaker Autopilot. The testing split will be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df.sample(frac=0.8,random_state=200)\n",
    "test_data = df.drop(train_data.index)\n",
    "#Drop the target column from the test data\n",
    "test_data_no_target = test_data.drop(columns=['risk_infection'])\n",
    "\n",
    "print('Total Record Count of the Dataset: ' + str(df.shape[0]))\n",
    "print('Training Data Record Count: ' + str(train_data.shape[0]))\n",
    "print('Testing Data Record Count: ' + str(test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training and test files to S3 locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'train_data.csv';\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print('Train data uploaded to: ' + train_data_s3_path)\n",
    "\n",
    "test_file = 'test_data.csv';\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print('Test data uploaded to: ' + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the SageMaker Autopilot Job<a name=\"SetupAPjob\"></a>\n",
    "\n",
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset. \n",
    "\n",
    "The required inputs for invoking a Autopilot job are:\n",
    "* Amazon S3 location for input dataset and for all output artifacts\n",
    "* Name of the column of the dataset you want to predict (`risk_infection` in this case) \n",
    "* An IAM role\n",
    "\n",
    "Currently Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order by name, is expected to have a header row.\n",
    "\n",
    "You can also specify the type of problem you want to solve with your dataset (`Regression, MulticlassClassification, BinaryClassification`). In case you are not sure, SageMaker Autopilot will infer the problem type based on statistics of the target column (the column you want to predict). \n",
    "\n",
    "Because the target attribute, ```risk_infection```, is a real value, our model will be performing regression. In this example we will let AutoPilot infer the type of problem for us.\n",
    "\n",
    "You have the option to limit the running time of a SageMaker Autopilot job by providing either the maximum number of pipeline evaluations or candidates (one pipeline evaluation is called a `Candidate` because it generates a candidate model) or providing the total time allocated for the overall Autopilot job. Under default settings, this job takes about four hours to run. This varies between runs because of the nature of the exploratory process Autopilot uses to find optimal training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_config = [{\n",
    "      'DataSource': {\n",
    "        'S3DataSource': {\n",
    "          'S3DataType': 'S3Prefix',\n",
    "          'S3Uri': 's3://{}/{}/train'.format(bucket,prefix)\n",
    "        }\n",
    "      },\n",
    "      'TargetAttributeName': 'risk_infection'\n",
    "    }\n",
    "  ]\n",
    "\n",
    "output_data_config = {\n",
    "    'S3OutputPath': 's3://{}/{}/output'.format(bucket,prefix)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the SageMaker Autopilot Job\n",
    "\n",
    "You can now launch the Autopilot job by calling the `create_auto_ml_job` API. We limit the number of candidates to 20 so that the job finishes in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "auto_ml_job_name = 'automl-covid19-' + timestamp_suffix\n",
    "print('AutoMLJobName: ' + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(AutoMLJobName=auto_ml_job_name,\n",
    "                      InputDataConfig=input_data_config,\n",
    "                      OutputDataConfig=output_data_config,\n",
    "                      AutoMLJobConfig={'CompletionCriteria':\n",
    "                                       {'MaxCandidates': 20}\n",
    "                                      },\n",
    "                      RoleArn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking SageMaker Autopilot job progress\n",
    "SageMaker Autopilot job consists of the following high-level steps : \n",
    "* Analyzing Data, where the dataset is analyzed and Autopilot comes up with a list of ML pipelines that should be tried out on the dataset. The dataset is also split into train and validation sets.\n",
    "* Feature Engineering, where Autopilot performs feature transformation on individual features of the dataset as well as at an aggregate level.\n",
    "* Model Tuning, where the top performing pipeline is selected along with the optimal hyperparameters for the training algorithm (the last stage of the pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('JobStatus - Secondary Status')\n",
    "print('------------------------------')\n",
    "\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response['AutoMLJobStatus']\n",
    "    \n",
    "    print (describe_response['AutoMLJobStatus'] + \" - \" + describe_response['AutoMLJobSecondaryStatus'])\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results\n",
    "\n",
    "Use the `describe_auto_ml_job` API to look up the best candidate selected by the SageMaker Autopilot job based on the objective metric selected by SageMaker AutoPilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)['BestCandidate']\n",
    "best_candidate_name = best_candidate['CandidateName']\n",
    "print(best_candidate)\n",
    "print('\\n')\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\"FinalAutoMLJobObjectiveMetricName: \" + best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'])\n",
    "print(\"FinalAutoMLJobObjectiveMetricValue: \" + str(best_candidate['FinalAutoMLJobObjectiveMetric']['Value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch inference using the best candidate<a name=\"BatchInf\"></a>\n",
    "\n",
    "Create a model using the best candidate selected by SageMaker AutoPilot and create a batch transform job as our test data has more than 100K records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "model_name = best_candidate_name + timestamp_suffix + \"-model\"\n",
    "model_arn = sm.create_model(Containers=best_candidate['InferenceContainers'],\n",
    "                            ModelName=model_name,\n",
    "                            ExecutionRoleArn=role)\n",
    "print('Model ARN corresponding to the best candidate is : {}'.format(model_arn['ModelArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_job_name = 'automl-covid19-transform-' + timestamp_suffix\n",
    "\n",
    "transform_input = {\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': test_data_s3_path\n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'text/csv',\n",
    "        'CompressionType': 'None',\n",
    "        'SplitType': 'Line'\n",
    "    }\n",
    "\n",
    "transform_output = {\n",
    "        'S3OutputPath': 's3://{}/{}/inference-results'.format(bucket,prefix),\n",
    "    }\n",
    "\n",
    "transform_resources = {\n",
    "        'InstanceType': 'ml.m5.4xlarge',\n",
    "        'InstanceCount': 1\n",
    "    }\n",
    "\n",
    "sm.create_transform_job(TransformJobName = transform_job_name,\n",
    "                        ModelName = model_name,\n",
    "                        TransformInput = transform_input,\n",
    "                        TransformOutput = transform_output,\n",
    "                        TransformResources = transform_resources\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('JobStatus')\n",
    "print('----------')\n",
    "\n",
    "\n",
    "describe_response = sm.describe_transform_job(TransformJobName = transform_job_name)\n",
    "job_run_status = describe_response['TransformJobStatus']\n",
    "print (job_run_status)\n",
    "\n",
    "while job_run_status not in ('Failed', 'Completed', 'Stopped'):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName = transform_job_name)\n",
    "    job_run_status = describe_response['TransformJobStatus']\n",
    "    print (job_run_status)\n",
    "    sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_key = '{}/inference-results/test_data.csv.out'.format(prefix);\n",
    "local_inference_results_path = 'inference_results.csv'\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "inference_results_bucket = s3.Bucket(session.default_bucket())\n",
    "\n",
    "inference_results_bucket.download_file(s3_output_key, local_inference_results_path);\n",
    "\n",
    "inference_data = pd.read_csv(local_inference_results_path, sep=';', header=None)\n",
    "#Keep the output on one page\n",
    "pd.set_option('display.max_rows', 10)\n",
    "inference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are performing regression for a continuous value (i.e.linear regression) then you may use metrics such as MSE (mean square error) or RMSE (root mean square error) for evaluating the accuracy of a linear regression model. In our case we will use MSE. AutoPilot has also selected the best model based on MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.square(np.subtract(test_data.reset_index()['risk_infection'],inference_data[0])).mean()\n",
    "print('Mean Square Error: {0:.2f}'.format(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the MSE calculation, visualization in form of charts gives us a better idea about the model. In this case we have ploted the line graphs for the true values vs predicted values. For better readability we have plotted the last 100 values. However keep in mind that the MSE is calculated for the complete test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 5\n",
    "\n",
    "plt.plot(test_data.reset_index().tail(100).index.values, test_data.reset_index()['risk_infection'].tail(100), color = \"red\")\n",
    "plt.plot(test_data.reset_index().tail(100).index.values, inference_data[0].tail(100), color = \"green\")\n",
    "plt.title(\"True Probability vs Predicted Probability (Mean Square Error:{0:.2f})\".format(MSE))\n",
    "plt.xlabel(\"Person Index\")\n",
    "plt.ylabel(\"Infection Risk Probability\")\n",
    "plt.legend(['True Probability','Predicted Probability'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. This operation deletes all the generated models and the auto-generated notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 = boto3.resource('s3')\n",
    "#s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "#job_outputs_prefix = '{}/output/{}'.format(prefix, auto_ml_job_name)\n",
    "#s3_bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
